{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and functions, does nothing\n",
    "\n",
    "from functools import partial\n",
    "from IPython.display import Image, display, Video, Image, HTML\n",
    "from matplotlib import rcParams\n",
    "from prettytable import PrettyTable\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import constants as cst\n",
    "import glob\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import lib.plot_builder as plot_builder\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "import pathlib\n",
    "import random\n",
    "import sklearn\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "\n",
    "def print_table(data, fields=None):\n",
    "    if len(data) == 0:\n",
    "        return\n",
    "    if len(data[0]) < len(fields):\n",
    "        fields = fields[:len(data[0])]\n",
    "    while len(data[0]) > len(fields):\n",
    "        fields.append('*')\n",
    "    t = PrettyTable()\n",
    "    for row in data:\n",
    "        t.add_row(row)\n",
    "    t.align = 'l'\n",
    "    if fields != None:\n",
    "        t.field_names = fields\n",
    "    print(t)\n",
    "\n",
    "def read_dataset(f):\n",
    "    acc = None\n",
    "    precision = None\n",
    "    recall = None\n",
    "    f1score = None\n",
    "    with open(f) as f2:\n",
    "        data = json.load(f2)\n",
    "        acc = round(100*data['score']['accuracy'][0], 1)\n",
    "        precision = round(100*data['score']['precision'][0], 1)\n",
    "        recall = round(100*data['score']['recall'][0], 1)\n",
    "        f1score = round(100*data['score']['f1score'][0], 1)\n",
    "\n",
    "    cm = f.replace('datasets/', 'plots/').replace('.json', '-cm.png')\n",
    "    fi = f.replace('datasets/', 'plots/').replace('.json', '-fi.png')\n",
    "\n",
    "    return acc, precision, recall, f1score, cm, fi\n",
    "\n",
    "def img(path, width=400):\n",
    "    rnd = random.randint(0,2e9)\n",
    "    return f\"\"\"<img src=\"{path}?nocache={rnd}\" style=\"width:{width}px; \"></img>\"\"\"\n",
    "\n",
    "def latex_table(table, header):\n",
    "\n",
    "    if len(table) == 0:\n",
    "        return\n",
    "    if len(table[0]) < len(header):\n",
    "        header = header[:len(table[0])]\n",
    "    while len(table[0]) > len(header):\n",
    "        header.append('*')\n",
    "\n",
    "    header_bold = [\"\\\\textbf{\"+t+\"}\" for t in header]\n",
    "    table2 = [header_bold]\n",
    "    table2.extend(table)\n",
    "    latex_table = \"\"\"\\\\begin{tabular}{lrrrr}\n",
    "\"\"\"\n",
    "    rows = [\" & \".join(map(str,row)) for row in table2]\n",
    "    latex_table += \"\".join([\"    \" + row + \" \\\\\\\\\\n\" for row in rows])\n",
    "    latex_table += \"\"\"\\\\end{tabular}\"\"\"\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "def dataset_name_to_friendly_name(name):\n",
    "    root_dataset = name.replace('-netflow1000', '').replace('-netflow100', '').replace('-netflow10', '').replace('-netflows', '').replace('-netflow', '').replace('-defended', '').replace('-google', '')\n",
    "    nicename = name.replace('-netflow1000', \"NF 0.1%\").replace('-netflow100', \"NF   1%\").replace('-netflow10', \"NF  10%\").replace('-netflow', \"NF 100%\").replace('/', ' ').replace('-defended', ' (defended)').replace('-google', '(Google dest filter)')\n",
    "    variant = nicename.replace(root_dataset, '')\n",
    "    return root_dataset, nicename, variant\n",
    "\n",
    "def find_pos(name, features):\n",
    "    i = 0\n",
    "    while i<len(features):\n",
    "        if features[i].strip() == name:\n",
    "            return i\n",
    "        i += 1\n",
    "    print(\"Couldn't find\", name)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads datasets of interest\n",
    "files = glob.glob(\"datasets/*.json\")\n",
    "files = [f for f in files]\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_data =  [['pcap', 319914, 95.8], ['100%', 26537, 90.5], ['10%', 3034, 66.4], ['1%', 923, 41.7], ['0.1%', 433, 16.8]]\n",
      "\\begin{tabular}{lrrrr}\n",
      "    \\textbf{Dataset} & \\textbf{Sampling} & \\textbf{F1 Score} \\\\\n",
      "    pcap & 319914 & 95.8 \\\\\n",
      "    100\\% & 26537 & 90.5 \\\\\n",
      "    10\\% & 3034 & 66.4 \\\\\n",
      "    1\\% & 923 & 41.7 \\\\\n",
      "    0.1\\% & 433 & 16.8 \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# Table 3\n",
    "# Print the table summary with Dataset + Accuracy (just the netflows)\n",
    "tuples = []\n",
    "\n",
    "def size(dataset_path, path=\"**/nfcapd*\", name=\"\"):\n",
    "    files = glob.glob(dataset_path + path, recursive=True)\n",
    "    \n",
    "    sizes = dict()\n",
    "    for p in files:\n",
    "        f = p.replace(dataset_path, '')\n",
    "        parts = f.split('/')\n",
    "        url = parts[0]\n",
    "\n",
    "        if not url in sizes:\n",
    "            sizes[url] = []\n",
    "\n",
    "        size = os.path.getsize(p)\n",
    "        sizes[url].append(size)\n",
    "\n",
    "    keys = list(sizes.keys())\n",
    "    means = []\n",
    "    \n",
    "    for k in keys:\n",
    "        mean = round(np.mean(sizes[k]), 2)\n",
    "        means.append(mean)\n",
    "\n",
    "    dataset = dataset_path.replace(\"../../cf-clusters-datasets/\", \"\").replace('netflows_1000', \"0.1%\").replace('netflows_100', \"  1%\").replace('netflows_10', \" 10%\").replace('netflows', \"100%\").replace('/', ' ')\n",
    "\n",
    "    return [dataset, round(np.min(means)), round(np.mean(means)), round(np.max(means))]\n",
    "\n",
    "def getf1(f):\n",
    "    acc, precision, recall, f1score, cm, fi = read_dataset(f)\n",
    "    return f1score\n",
    "\n",
    "\n",
    "sizes = dict()\n",
    "sizes['pcap'] = size(\"../../cf-clusters-datasets/quic-100p-150/pcaps/\", path=\"**/*.pcap\")\n",
    "sizes['100%'] = size(\"../../cf-clusters-datasets/quic-100p-150/netflows/\")\n",
    "sizes['10%'] = size(\"../../cf-clusters-datasets/quic-100p-150/netflows_10/\")\n",
    "sizes['1%'] = size(\"../../cf-clusters-datasets/quic-100p-150/netflows_100/\")\n",
    "sizes['0.1%'] = size(\"../../cf-clusters-datasets/quic-100p-150/netflows_1000/\")\n",
    "\n",
    "accs = dict()\n",
    "accs['pcap'] = getf1(\"../randomforests/datasets/quic-100p-150.json\")\n",
    "accs['100%'] = getf1(\"./datasets/quic-100p-150-netflow.json\")\n",
    "accs['10%'] = getf1(\"./datasets/quic-100p-150-netflow10.json\")\n",
    "accs['1%'] = getf1(\"./datasets/quic-100p-150-netflow100.json\")\n",
    "accs['0.1%'] = getf1(\"./datasets/quic-100p-150-netflow1000.json\")\n",
    "\n",
    "table_data = []\n",
    "for key in sizes.keys():\n",
    "    table_data.append([key, sizes[key][2], accs[key]])\n",
    "\n",
    "print(f\"table_data = \", table_data)\n",
    "print(latex_table([[t[0].replace('%', '\\\\%'), t[1], t[2]] for t in table_data], header=[\"Dataset\", \"Sampling\", \"F1 Score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_data =  [['100%', 40508.2, 53.1], ['10%', 4345.4, 33.1], ['1%', 432.7, 21.6], ['0.1%', 43.1, 8.6]]\n",
      "\\begin{tabular}{lrrrr}\n",
      "    \\textbf{Sampling} & \\textbf{Cost [kB/sample]} & \\textbf{F1 Score} \\\\\n",
      "    100\\% & 40508.2 & 53.1 \\\\\n",
      "    10\\% & 4345.4 & 33.1 \\\\\n",
      "    1\\% & 432.7 & 21.6 \\\\\n",
      "    0.1\\% & 43.1 & 8.6 \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# Defended netflows\n",
    "\n",
    "cost = dict()\n",
    "cost['100%'] = [41480379.910333335, 19176125091471.715]\n",
    "cost['10%'] = [4449720.7485, 71816947398.78859]\n",
    "cost['1%'] = [443051.086, 1554101157.4342709]\n",
    "cost['0.1%'] = [44110.254, 27883060.782484]\n",
    "\n",
    "accs = dict()\n",
    "accs['100%'] = getf1(\"./datasets/quic-100p-150-netflow-nototsize.json\")\n",
    "accs['10%'] = getf1(\"./datasets/quic-100p-150-netflow10-nototsize.json\")\n",
    "accs['1%'] = getf1(\"./datasets/quic-100p-150-netflow100-nototsize.json\")\n",
    "accs['0.1%'] = getf1(\"./datasets/quic-100p-150-netflow1000-nototsize.json\")\n",
    "\n",
    "table_data = []\n",
    "for key in cost.keys():\n",
    "    table_data.append([key, round(cost[key][0]/1024, 1), accs[key]])\n",
    "\n",
    "print(f\"table_data = \", table_data)\n",
    "print(latex_table([[t[0].replace('%', '\\\\%'), t[1], t[2]] for t in table_data], header=[\"Sampling\", \"Cost [kB/sample]\", \"F1 Score\"]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
