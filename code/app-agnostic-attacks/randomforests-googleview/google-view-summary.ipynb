{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and functions, does nothing\n",
    "\n",
    "from functools import partial\n",
    "from IPython.display import Image, display, Video, Image, HTML\n",
    "from matplotlib import rcParams\n",
    "from prettytable import PrettyTable\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import constants as cst\n",
    "import glob\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import lib.plot_builder as plot_builder\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "import pathlib\n",
    "import random\n",
    "import sklearn\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "\n",
    "def print_table(data, fields=None):\n",
    "    t = PrettyTable()\n",
    "    for row in data:\n",
    "        t.add_row(row)\n",
    "    t.align = 'l'\n",
    "    if fields != None:\n",
    "        t.field_names = fields\n",
    "    print(t)\n",
    "    \n",
    "def read_dataset(f):\n",
    "    acc = None\n",
    "    precision = None\n",
    "    recall = None\n",
    "    f1score = None\n",
    "    with open(f) as f2:\n",
    "        data = json.load(f2)\n",
    "        acc = round(100*data['score']['accuracy'][0], 1)\n",
    "        precision = round(100*data['score']['precision'][0], 1)\n",
    "        recall = round(100*data['score']['recall'][0], 1)\n",
    "        f1score = round(100*data['score']['f1score'][0], 1)\n",
    "\n",
    "    cm = f.replace('datasets/', 'plots/').replace('.json', '-cm.png')\n",
    "    fi = f.replace('datasets/', 'plots/').replace('.json', '-fi.png')\n",
    "\n",
    "    return acc, precision, recall, f1score, cm, fi\n",
    "def img(path, width=400):\n",
    "    rnd = random.randint(0,2e9)\n",
    "    return f\"\"\"<img src=\"{path}?nocache={rnd}\" style=\"width:{width}px; \"></img>\"\"\"\n",
    "\n",
    "def latex_table(table, header):\n",
    "    header_bold = [\"\\\\textbf{\"+t+\"}\" for t in header]\n",
    "    table2 = [header_bold]\n",
    "    table2.extend(table)\n",
    "    latex_table = \"\"\"\\\\begin{tabular}{lrrrr}\n",
    "\"\"\"\n",
    "    rows = [\" & \".join(map(str,row)) for row in table2]\n",
    "    latex_table += \"\".join([\"    \" + row + \" \\\\\\\\\\n\" for row in rows])\n",
    "    latex_table += \"\"\"\\\\end{tabular}\"\"\"\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "def dataset_name_to_friendly_name(name):\n",
    "    root_dataset = name.replace('-netflow1000', '').replace('-netflow100', '').replace('-netflow10', '').replace('-netflows', '').replace('-netflow', '').replace('-defended', '').replace('-google', '')\n",
    "    nicename = name.replace('-netflow1000', \"NF 0.1%\").replace('-netflow100', \"NF   1%\").replace('-netflow10', \"NF  10%\").replace('-netflow', \"NF 100%\").replace('/', ' ').replace('-defended', ' (defended)').replace('-google', '(Google dest filter)')\n",
    "    variant = nicename.replace(root_dataset, '')\n",
    "    return root_dataset, nicename, variant\n",
    "\n",
    "def find_pos(name, features):\n",
    "    i = 0\n",
    "    while i<len(features):\n",
    "        if features[i].strip() == name:\n",
    "            return i\n",
    "        i += 1\n",
    "    print(\"Couldn't find\", name)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads datasets of interest\n",
    "files = glob.glob(\"datasets/*.json\")\n",
    "files = [f for f in files]\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"row\" style=\"text-align:left\">\n",
       "            <table>\n",
       "            <td style=\"text-align: left\">\n",
       "            <h2>datasets/quic-100p-150</h2>\n",
       "            <h3>Accuracy 78.4%</h3>\n",
       "            <img src=\"plots/quic-100p-150-google-cm.png?nocache=426257846\" style=\"width:400px; \"></img><img src=\"plots/quic-100p-150-google-fi.png?nocache=1168840762\" style=\"width:400px; \"></img>\n",
       "            <td>\n",
       "            </td>\n",
       "            </tr>\n",
       "            </table>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in files:\n",
    "    if not \"-google\" in f:\n",
    "        continue\n",
    "\n",
    "\n",
    "    dataset = f.replace('dataset/', '').replace('.json', '')\n",
    "    acc, _, _, _, cm, fi = read_dataset(f)\n",
    "    dataset_name, _, variant = dataset_name_to_friendly_name(dataset)\n",
    "    \n",
    "    display(HTML(f\"\"\"\n",
    "    <div class=\"row\" style=\"text-align:left\">\n",
    "            <table>\n",
    "            <td style=\"text-align: left\">\n",
    "            <h2>{dataset_name}</h2>\n",
    "            <h3>Accuracy {acc}%</h3>\n",
    "            \"\"\" + img(cm) + img(fi) + f\"\"\"\n",
    "            <td>\n",
    "            </td>\n",
    "            </tr>\n",
    "            </table>\n",
    "    </div>\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/quic-100p-150-google.json\n",
      "+---------------+----------------------+------+\n",
      "| Dataset       | Variant              | Acc  |\n",
      "+---------------+----------------------+------+\n",
      "| quic-100p-150 | (Google dest filter) | 78.4 |\n",
      "+---------------+----------------------+------+\n",
      "\\begin{tabular}{lrrrr}\n",
      "    \\textbf{Dataset} & \\textbf{Variant} & \\textbf{F1 Score} \\\\\n",
      "    \\texttt{quic-100p-150} & (Google dest filter) & 78.4 \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# Table 2\n",
    "# Print the table summary with Dataset + Accuracy (just the google-destination filter)\n",
    "tuples = []\n",
    "\n",
    "for f in files:\n",
    "    if not \"-google\" in f:\n",
    "        continue\n",
    "\n",
    "    print(f)\n",
    "\n",
    "    dataset = f.replace('datasets/', '').replace('.json', '')\n",
    "\n",
    "    acc, _, _, _, cm, fi = read_dataset(f)\n",
    "    dataset_name, _, variant = dataset_name_to_friendly_name(dataset)\n",
    "    \n",
    "    tuples.append([dataset_name, variant, acc])\n",
    "\n",
    "print_table(tuples, fields=[\"Dataset\", \"Variant\", \"Acc\"])\n",
    "print(latex_table([[f\"\\\\texttt{{{t[0]}}}\", t[1], t[2]] for t in tuples], header=[\"Dataset\", \"Variant\", \"F1 Score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-------+--------+---------+\n",
      "| Dataset                  | Min   | Mean   | Max     |\n",
      "+--------------------------+-------+--------+---------+\n",
      "| quic-150                 | 24.46 | 312.42 | 2101.94 |\n",
      "| quic-150 (Google filter) | 0.02  | 112.15 | 1066.75 |\n",
      "+--------------------------+-------+--------+---------+\n",
      "\\begin{tabular}{lrrrr}\n",
      "    \\textbf{Dataset} & \\textbf{Mean [kB]} \\\\\n",
      "    \\texttt{quic-150} & 312.42 \\\\\n",
      "    \\texttt{quic-150 (Google filter)} & 112.15 \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# Google Filter PCAP sizes\n",
    "\n",
    "#!/usr/bin/python3\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def getsizesnetflows(name, dataset_path):\n",
    "    return getsizespcap(name, dataset_path, filterstr=\"**/nfcapd*\")\n",
    "\n",
    "def getsizespcap(name, dataset_path, filterstr=\"**/*.pcap\"):\n",
    "    pcaps = glob.glob(dataset_path + filterstr, recursive=True)\n",
    "\n",
    "    sizes = dict()\n",
    "    for p in pcaps:\n",
    "        f = p.replace(dataset_path, '')\n",
    "        parts = f.split('/')\n",
    "        url = parts[0]\n",
    "\n",
    "        if not url in sizes:\n",
    "            sizes[url] = []\n",
    "\n",
    "        size = os.path.getsize(p)\n",
    "        sizes[url].append(size)\n",
    "\n",
    "    keys = list(sizes.keys())\n",
    "    means = []\n",
    "    \n",
    "    for k in keys:\n",
    "        mean = round(np.mean(sizes[k])/1024, 2)\n",
    "        means.append(mean)\n",
    "\n",
    "    return [name, round(np.min(means),2), round(np.mean(means),2), round(np.max(means),2)]\n",
    "\n",
    "table = []\n",
    "table.append(getsizespcap('quic-150', '../../cf-clusters-datasets/quic-100p-150/pcaps/'))\n",
    "table.append(getsizespcap('quic-150 (Google filter)', '../../cf-clusters-datasets/quic-100p-150-google/pcaps/'))\n",
    "#table.append(getsizesnetflows('quic-150 (Google filter + netflows 100%)', '../../cf-clusters-datasets/quic-100p-338-google/netflows/'))\n",
    "#table.append(getsizesnetflows('quic-150 (Google filter + netflows 10%)', '../../cf-clusters-datasets/quic-100p-338-google/netflows_10/'))\n",
    "#table.append(getsizesnetflows('quic-150 (Google filter + netflows 1%)', '../../cf-clusters-datasets/quic-100p-338-google/netflows_100/'))\n",
    "#table.append(getsizesnetflows('quic-150 (Google filter + netflows 0.1%)', '../../cf-clusters-datasets/quic-100p-338-google/netflows_1000/'))\n",
    "\n",
    "\n",
    "print_table(table, fields=[\"Dataset\", \"Min\", \"Mean\", \"Max\"])\n",
    "print(latex_table([[f\"\\\\texttt{{{t[0]}}}\", t[2],] for t in table], header=[\"Dataset\", \"Mean [kB]\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------+----------+\n",
      "| Dataset  | Variant       | Sampling | F1 Score |\n",
      "+----------+---------------+----------+----------+\n",
      "| quic-150 |               |          | 95.8     |\n",
      "|          | (Google view) |          | 78.4     |\n",
      "+----------+---------------+----------+----------+\n",
      "\\begin{tabular}{lrrrr}\n",
      "    \\textbf{Dataset} & \\textbf{Variant} & \\textbf{Sampling} & \\textbf{F1 Score} \\\\\n",
      "    \\texttt{quic-150} &  &  & 95.8 \\\\\n",
      "    \\texttt{} & (Google view) &  & 78.4 \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# I think this is invalid. We're comparing apple and oranges\n",
    "\n",
    "# loads datasets of interest\n",
    "files_google_netflows = glob.glob(\"../randomforests-netflows/datasets/*.json\")\n",
    "files_google_netflows.sort()\n",
    "\n",
    "tuples = []\n",
    "\n",
    "acc, precision, recall, f1score, cm, fi = read_dataset('../randomforests/datasets/quic-100p-150.json')\n",
    "tuples.append(['quic-150', \"\", \"\", f1score])\n",
    "\n",
    "acc, precision, recall, f1score, cm, fi = read_dataset('./datasets/quic-100p-150-google.json')\n",
    "tuples.append(['', \"(Google view)\", \"\", f1score])\n",
    "\n",
    "for f in files_google_netflows:\n",
    "    continue\n",
    "    if \"google\" not in f or \"-no\" in f:\n",
    "        continue\n",
    "\n",
    "    dataset = f.replace('../randomforests-netflows/datasets/', '').replace('.json', '').replace(\"-google\", \"\")\n",
    "    dataset_name, _, variant = dataset_name_to_friendly_name(dataset)\n",
    "    variant = variant.replace('NF ', '')\n",
    "    \n",
    "    acc, precision, recall, f1score, cm, fi = read_dataset(f)\n",
    "\n",
    "    tuples.append([\"\", \"(Google dest filter + netflow)\", variant, f1score])\n",
    "\n",
    "print_table(tuples, fields=[\"Dataset\", \"Variant\", \"Sampling\", \"F1 Score\"])\n",
    "print(latex_table([[f\"\\\\texttt{{{t[0]}}}\", t[1], t[2].replace('%', '\\\\%'), t[3]] for t in tuples], header=[\"Dataset\", \"Variant\", \"Sampling\", \"F1 Score\"]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
